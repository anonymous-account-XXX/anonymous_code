{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_latent_feature_path = 'your_path_to_data_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_with_latent_feature_path)\n",
    "data.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data columns: {data.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge race into 4 categories: white, black, hispanic, other\n",
    "race = data[\"race\"].tolist()\n",
    "new_race = []\n",
    "for i in race:\n",
    "  if \"white\" in i:\n",
    "    new_race.append(\"white\")\n",
    "  elif \"black\" in i:\n",
    "    new_race.append(\"black\")\n",
    "  elif \"hispanic\" in i:\n",
    "    new_race.append(\"hispanic\")\n",
    "  else:\n",
    "    new_race.append(\"other\")\n",
    "\n",
    "data[\"race\"] = new_race\n",
    "print(data[\"race\"].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill unknown values with mean\n",
    "\n",
    "def get_mean(data, column):\n",
    "    temp = data[data[column] != 'unknown']\n",
    "    return temp[column].astype(float).mean()\n",
    "\n",
    "# replace unknow in height and weight, age with 0\n",
    "data[\"bmi\"] = data[\"bmi\"].replace('unknown', get_mean(data, 'bmi'))\n",
    "data['height'] = data['height'].replace('unknown', get_mean(data, 'height'))\n",
    "data['weight'] = data['weight'].replace('unknown', get_mean(data, 'weight'))\n",
    "data['age'] = data['age'].replace('unknown', get_mean(data, 'age'))\n",
    "data['number_of_records'] = data['number_of_records'].replace('unknown', get_mean(data, 'number_of_records'))\n",
    "\n",
    "# separate blood pressure into systolic and diastolic\n",
    "data['blood_pressure'] = data['blood_pressure'].replace('unknown', '0/0')\n",
    "data['systolic'] = data['blood_pressure'].str.split('/').str[0]\n",
    "data['diastolic'] = data['blood_pressure'].str.split('/').str[1]\n",
    "\n",
    "data.drop(columns=['admit_time', 'discharge_time', 'blood_pressure'], inplace=True)\n",
    "\n",
    "number_columns = ['height', 'weight', 'systolic', 'diastolic', 'age', \"number_of_records\", \"bmi\"]\n",
    "for column in number_columns:\n",
    "    data[column] = data[column].astype(float)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('discharge_location', axis=1)\n",
    "y = data['discharge_location']\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "y_encoded = y.map({'home': 0, 'other': 1, 'died': 2})\n",
    "\n",
    "seeds = [42,126,88,999,255]\n",
    "X_trains = []\n",
    "y_trains = []\n",
    "X_tests = []\n",
    "y_tests = []\n",
    "\n",
    "for seed in seeds:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=seed, stratify=y_encoded)\n",
    "    X_trains.append(X_train)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test)\n",
    "    y_tests.append(y_test)\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print()\n",
    "    print(f\"Target distribution in train: {y_train.value_counts(normalize=True)}\")\n",
    "    print(f\"Target distribution in test: {y_test.value_counts(normalize=True)}\")\n",
    "    print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deal with data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed_trains = []\n",
    "y_processed_trains = []\n",
    "for X_encoded_train, y_encoded_train, seed in zip(X_trains, y_trains, seeds):\n",
    "  smote = SMOTE(random_state=seed)\n",
    "  X_encoded_train_smote, y_encoded_train_smote = smote.fit_resample(X_encoded_train, y_encoded_train)\n",
    "  print(f\"Seed: {seed}\")\n",
    "  print(f\"SMOTE train shape: {X_encoded_train_smote.shape}\")\n",
    "  print(f\"SMOTE train discharge_location distribution: {y_encoded_train_smote.value_counts()}\")\n",
    "  print(\"-------------------------------------------------\")\n",
    "  X_processed_trains.append(X_encoded_train_smote)\n",
    "  y_processed_trains.append(y_encoded_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creat baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed_trains_wolf = []\n",
    "X_tests_wolf = []\n",
    "\n",
    "for X_encoded_train, X_encoded_test, seed in zip(X_processed_trains, X_tests, seeds):\n",
    "  X_train_wolf = X_encoded_train.drop(columns=[\"social_support_Weak\"])\n",
    "  X_test_wolf = X_encoded_test.drop(columns=[\"social_support_Weak\"])\n",
    "  X_processed_trains_wolf.append(X_train_wolf)\n",
    "  X_tests_wolf.append(X_test_wolf)\n",
    "  print(X_encoded_train.columns)\n",
    "  print()\n",
    "  print( X_train_wolf.columns)\n",
    "  print()\n",
    "  print(X_test.columns)\n",
    "  print()\n",
    "  print(X_test_wolf.columns)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "F1_scores = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains, y_processed_trains, X_tests, y_tests, seeds):\n",
    "    \n",
    "    param_grid = {'C': [0.1, 1, 10, 100], 'max_iter': [1000, 10000], 'solver': ['liblinear', 'lbfgs'], 'penalty': ['l1', 'l2', 'elasticnet']}\n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # train model with best hyperparameters\n",
    "    model = LogisticRegression(C=grid.best_params_['C'], max_iter=grid.best_params_['max_iter'])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "    F1_scores.append(f1_scores)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"#################################################\")\n",
    "print(\"Summary of model with latent feature\")\n",
    "print(f\"Average accuracy: {sum(accuracies)/len(accuracies)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores)/len(F1_scores)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without latent features(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "F1_scores = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains_wolf, y_processed_trains, X_tests_wolf, y_tests, seeds):\n",
    "    \n",
    "    param_grid = {'C': [0.1, 1, 10, 100], 'max_iter': [1000, 10000], 'solver': ['liblinear', 'lbfgs'], 'penalty': ['l1', 'l2', 'elasticnet']}\n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # train model with best hyperparameters\n",
    "    model = LogisticRegression(C=grid.best_params_['C'], max_iter=grid.best_params_['max_iter'])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "    F1_scores.append(f1_scores)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"#################################################\")\n",
    "print(\"Summary of model without latent feature (baseline)\")\n",
    "print(f\"Average accuracy: {sum(accuracies)/len(accuracies)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores)/len(F1_scores)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_mlp = []\n",
    "F1_scores_mlp = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains, y_processed_trains, X_tests, y_tests, seeds):\n",
    "      \n",
    "      param_grid = {'hidden_layer_sizes': [(100,), (200,), (300,)], 'activation': ['relu', 'tanh', 'logistic'], 'solver': ['adam', 'sgd'], 'max_iter': [1000, 10000]}\n",
    "      grid = GridSearchCV(MLPClassifier(), param_grid, verbose=0)\n",
    "      grid.fit(X_train, y_train)\n",
    "      print(f\"Seed: {seed}\")\n",
    "      print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "      print(\"-------------------------------------------------\")\n",
    "      \n",
    "      # train model with best hyperparameters\n",
    "      model = MLPClassifier(hidden_layer_sizes=grid.best_params_['hidden_layer_sizes'], activation=grid.best_params_['activation'], solver=grid.best_params_['solver'], max_iter=grid.best_params_['max_iter'])\n",
    "      model.fit(X_train, y_train)\n",
    "      y_pred = model.predict(X_test)\n",
    "      \n",
    "      accuracy = accuracy_score(y_test, y_pred)\n",
    "      accuracies_mlp.append(accuracy)\n",
    "      f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "      F1_scores_mlp.append(f1_scores)\n",
    "      print(classification_report(y_test, y_pred))\n",
    "      print(\"\\n\")\n",
    "\n",
    "print(\"#################################################\")\n",
    "print(\"Summary of model with latent feature\")\n",
    "print(f\"Average accuracy: {sum(accuracies_mlp)/len(accuracies_mlp)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies_mlp)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores_mlp)/len(F1_scores_mlp)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores_mlp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without latent features(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_mlp_wolf = []\n",
    "F1_scores_mlp_wolf = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains_wolf, y_processed_trains, X_tests_wolf, y_tests, seeds):\n",
    "        \n",
    "        param_grid = {'hidden_layer_sizes': [(100,), (200,), (300,)], 'activation': ['relu', 'tanh', 'logistic'], 'solver': ['adam', 'sgd'], 'max_iter': [1000, 10000]}\n",
    "        grid = GridSearchCV(MLPClassifier(), param_grid, verbose=0)\n",
    "        grid.fit(X_train, y_train)\n",
    "        print(f\"Seed: {seed}\")\n",
    "        print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        \n",
    "        # train model with best hyperparameters\n",
    "        model = MLPClassifier(hidden_layer_sizes=grid.best_params_['hidden_layer_sizes'], activation=grid.best_params_['activation'], solver=grid.best_params_['solver'], max_iter=grid.best_params_['max_iter'])\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies_mlp_wolf.append(accuracy)\n",
    "        f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "        F1_scores_mlp_wolf.append(f1_scores)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "print(\"#################################################\")\n",
    "print(\"Summary of model without latent feature (baseline)\")\n",
    "print(f\"Average accuracy: {sum(accuracies_mlp_wolf)/len(accuracies_mlp_wolf)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies_mlp_wolf)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores_mlp_wolf)/len(F1_scores_mlp_wolf)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores_mlp_wolf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_rf = []\n",
    "F1_scores_rf = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains, y_processed_trains, X_tests, y_tests, seeds):\n",
    "        \n",
    "        param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9], 'verbose': [0]}\n",
    "        grid = GridSearchCV(RandomForestClassifier(), param_grid, verbose=0, n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        print(f\"Seed: {seed}\")\n",
    "        print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=grid.best_params_['max_depth'])\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies_rf.append(accuracy)\n",
    "        f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "        F1_scores_rf.append(f1_scores)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "print(\"#################################################\")\n",
    "print(\"Summary of Random Forest model with latent feature\")\n",
    "print(f\"Average accuracy: {sum(accuracies_rf)/len(accuracies_rf)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies_rf)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores_rf)/len(F1_scores_rf)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores_rf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without latent features(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_rf_wolf = []\n",
    "F1_scores_rf_wolf = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains_wolf, y_processed_trains, X_tests_wolf, y_tests, seeds):\n",
    "          \n",
    "          param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9], 'verbose': [0]}\n",
    "          grid = GridSearchCV(RandomForestClassifier(), param_grid, verbose=0, n_jobs=-1)\n",
    "          grid.fit(X_train, y_train)\n",
    "          print(f\"Seed: {seed}\")\n",
    "          print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "          print(\"-------------------------------------------------\")\n",
    "          \n",
    "          model = RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=grid.best_params_['max_depth'])\n",
    "          model.fit(X_train, y_train)\n",
    "          y_pred = model.predict(X_test)\n",
    "          \n",
    "          accuracy = accuracy_score(y_test, y_pred)\n",
    "          accuracies_rf_wolf.append(accuracy)\n",
    "          f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "          F1_scores_rf_wolf.append(f1_scores)\n",
    "          print(classification_report(y_test, y_pred))\n",
    "          print(\"\\n\")\n",
    "          \n",
    "print(\"#################################################\")\n",
    "print(\"Summary of Random Forest model without latent feature (baseline)\")\n",
    "print(f\"Average accuracy: {sum(accuracies_rf_wolf)/len(accuracies_rf_wolf)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies_rf_wolf)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores_rf_wolf)/len(F1_scores_rf_wolf)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores_rf_wolf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_gbt = []\n",
    "F1_scores_gbt = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains, y_processed_trains, X_tests, y_tests, seeds):\n",
    "            \n",
    "            param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9], 'verbose': [0]}\n",
    "            grid = GridSearchCV(GradientBoostingClassifier(), param_grid, verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            print(f\"Seed: {seed}\")\n",
    "            print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "            print(\"-------------------------------------------------\")\n",
    "            \n",
    "            model = GradientBoostingClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=grid.best_params_['max_depth'])\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            accuracies_gbt.append(accuracy)\n",
    "            f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "            F1_scores_gbt.append(f1_scores)\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "print(\"#################################################\")\n",
    "print(\"Summary of Gradient Boosting model with latent feature\")\n",
    "print(f\"Average accuracy: {sum(accuracies_gbt)/len(accuracies_gbt)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies_gbt)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores_gbt)/len(F1_scores_gbt)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores_gbt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without latent features(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_gbt_wolf = []\n",
    "F1_scores_gbt_wolf = []\n",
    "\n",
    "for X_train, y_train, X_test, y_test, seed in zip(X_processed_trains_wolf, y_processed_trains, X_tests_wolf, y_tests, seeds):\n",
    "                \n",
    "                param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9], 'verbose': [0]}\n",
    "                grid = GridSearchCV(GradientBoostingClassifier(), param_grid, verbose=0)\n",
    "                grid.fit(X_train, y_train)\n",
    "                print(f\"Seed: {seed}\")\n",
    "                print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "                print(\"-------------------------------------------------\")\n",
    "                \n",
    "                model = GradientBoostingClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=grid.best_params_['max_depth'])\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                accuracies_gbt_wolf.append(accuracy)\n",
    "                f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "                F1_scores_gbt_wolf.append(f1_scores)\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                print(\"\\n\")\n",
    "                \n",
    "print(\"#################################################\")\n",
    "print(\"Summary of Gradient Boosting model without latent feature (baseline)\")\n",
    "print(f\"Average accuracy: {sum(accuracies_gbt_wolf)/len(accuracies_gbt_wolf)}\")\n",
    "print(f\"Standard deviation of accuracy: {np.std(accuracies_gbt_wolf)}\")\n",
    "print(f\"Average F1 score: {sum(F1_scores_gbt_wolf)/len(F1_scores_gbt_wolf)}\")\n",
    "print(f\"Standard deviation of F1 score: {np.std(F1_scores_gbt_wolf)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
